# LLMBench

LLMBench is a benchmarking suite for evaluating the performance and capabilities of various Large Language Models (LLMs) across different tasks. This folder contains scripts, data, and results for analyzing and comparing LLM outputs.

## Folder Structure

- `age_plot.py`  
  Script for plotting age-related statistics from evaluation data.

- `ARI_Flesh.py`  
  Calculates ARI (Automated Readability Index) and Flesch readability scores for model outputs.

- `GEval_test.py`  
  Runs GEval tests to assess LLM-generated responses.

- `print_GEVAL_stats.py`  
  Prints summary statistics from GEval evaluations.

- `wordanalysis.py`  
  Performs word-level analysis on LLM outputs.

- `LLM_interaction/`  
  Contains raw responses and prompts for various LLMs, including ChatGPT, Claude, Deepseek, Gemini, GROK, and Mistral.

- `scripts/`  
  Stores evaluation results and model scores in JSON format.

- `SVGs/`  
  Contains SVG visualizations generated by analysis scripts.

## Usage

1. **Prepare Data:**  
   Place LLM responses and prompts in the `LLM_interaction/` directory.

2. **Run Analysis:**  
   Use the provided Python scripts to analyze and visualize the results. For example:
   ```sh
   python ARI_Flesh.py
   ```

